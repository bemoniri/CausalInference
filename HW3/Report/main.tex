\documentclass{article}
\input{structure.tex} 
\title{تمرین کامپیوتری دوم - استنتاج علّی} 
\author{بهراد منیری\\95109564\\ \texttt{bemoniri@live.com}}
\date{دانشکده‌ی مهندسی برق - دانشگاه صنعتی شریف}
%----------------------------------------------------------------------------------------

\begin{document}
\maketitle
\section{بخش 
اول  - پیاده‌سازی الگوریتم 
\lr{pc}
}
در این بخش الگوریتم
\lr{PC}
ر ا با آزمون فرض استقلال شرطی مبتنی بر 
\lr{Partial Correlation}
پیاده سازی می‌کنیم. برای پیاده‌سازی از زبان پایتون استفاده کرده و گراف را به کمک کتابخانه‌ی 
\lr{networkx}
می‌سازیم. برای محاسبات نیز از کتابخانه‌های 
\lr{numpy}
و
\lr{sklearn}
استفاده می‌کنیم.
\subsection{پیاده‌سازی آزمون فرض}
برای پیاده‌سازی آزمون فرضیه،  تابع 
\lr{test}
را نوشتیم:
\lr{test (data, x, y, s)}


این تابع، ماتریس داده‌ها و اندیس $x$ ،$y$  و مجموعه‌ای از اندیس‌ها، $S$، را در ورودی گرفته و آزمون فرضیه‌ی
$$X\bigCI Y | S$$
را انجام می‌دهد و مقدار ‌
\lr{p-value}
را در خروجی بر می‌گرداند. هر اندیس نماینده‌ی ستون مورد نظر در ماتریس داده‌ها، یعنی یک متغیر است.
\subsection{پیاده‌سازی الگوریتم \lr{pc}}
تابع
\lr{pc}
پیاده‌سازی الگوریتم 
\lr{pc}
است که شبه‌کد آن ارائه شده.
\begin{latin}
pc (da, alpha)
\end{latin}
این تابع، ماتریس داده‌ها که هر ستون آن یک متغیر است و همچنین مقدار 
\lr{significance level}
یعنی
$\alpha$
را در ورودی دریافت کرده و اسکلت گراف مولد داده‌ها را تخمین می‌زند.
\subsection{
	بررسی عملکرد الگوریتم با دو 
\lr{Toy Example}
}
\label{toy}
برای بررسی عملکرد الگوریتم، چند
\lr{SCM}
گاوسی-خطی ساخته و الگوریتم را روی آنها اجرا می‌کنیم. از هر متغیر یک میلیون سمپل می‌سازیم.
\subsubsection{مثال اول}
\begin{equation}
\begin{cases}
x_0 = N_0\\
x_1 = x_0 + N_y\\
x_2 = x_1 + N_z\\
x_3 = 2x_2 + 3x_0 +  N_w\\
\end{cases}
N_1, N_2, N_3, N_4 \stackrel{iid}{\sim} Normal(0,1)
\end{equation}

برنامه‌ی ما، دو عبارت استقلال زیر را از داده استخراج می‌کند.
\begin{equation}
\begin{cases}
x_0 \bigCI x_2 | x_1\\
x_1 \bigCI x_3 | x_0, x_2\\
\end{cases}
\end{equation}
و دو یال متناظر را حذف می‌کند. 
شکل
\eqref{ex3}
گراف تخمین زده شده است. 
\textbf{تمام یال‌ها به درستی انتخاب شدند.}

\begin{figure}[h]
	\centering
		\includegraphics[scale=0.5]{pcex1.png}
	\caption{گراف تخمین‌زده‌شده برای مثال اول}
	\label{ex3}
\end{figure}
\subsubsection{مثال دوم}
\begin{equation}
\begin{cases}
x_0 = N_0\\
x_1 =  x_0 + N_1\\
x_2 =  2x_0 + N_2\\
x_3 =  3x_1 + N_3\\
x_4 =  4x_2 + N_4\\

\end{cases}
N_0, N_1, N_2, N_3, N_4 \stackrel{ind.}{\sim} Normal(0,\sigma_i)
\end{equation}

برنامه‌ی ما، دو عبارت استقلال زیر را از داده استخراج می‌کند.
\begin{equation}
\begin{cases}
x_1 \bigCI x_4 | x_2, x_3\\
x_0 \bigCI x_4 | x_1, x_2\\
x_2 \bigCI x_3 | x_1\\
x_0 \bigCI x_3 | x_1\\
\end{cases}
\end{equation}
و  یال‌های  متناظر را حذف می‌کند. 
شکل
\eqref{ex2}
گراف تخمین زده شده است. 
\textbf{تمام یال‌ها به درستی انتخاب شدند.}
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{pcex2.png}
	\caption{گراف تخمین‌زده‌شده برای مثال دوم}
	\label{ex2}
\end{figure}
\newpage
\section{بخش دوم - تخمین گراف مولد داده‌های ضمیمه}
به کمک تابع 
\lr{pc}
اسکلت گراف را تخمین می‌زنیم. این الگوریتم بعد از سطح 
$l = 6$
متوقف شده و ۴۱۵ یال در این گراف باقی می‌ماند.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{pcaout.png}
	\caption{خروجی الگوریتم}
	\label{out1}
\end{figure}

\section{بخش سوم - پیاده‌سازی الگوریتم
\lr{pc}
پایدار
}
با تغییری کوچک در الگوریتم 
\lr{pc}
می‌توان به الگوریتم 
\lr{pc-stable}
رسید. در الگوریتم پایدار، بعد از اینکه تصمیم به قطع یک یال گرفتیم، یال مورد نظر را ذخیره می‌کنیم و در آخر هر سطح، تمام یال‌هایی که باید حذف شوند را یک‌جا حذف می‌کنیم. در الگوریتم اصلی این امکان وجود دارد که به دلیل حذف اشتباه یک یال، برخی از آزمون فرض‌ها را دیگر انجام ندهید و به ای دلیل خطا به شدت گسترش پیدا کند. در روش پایدار تا حدی این اثر کم‌رنگ می‌شود.

\subsection{
بررسی عملکرد الگوریتم پایدار با دو 
\lr{Toy Example}
}

در این بخش الگوریتم 
\lr{pc-stable}
را بر روی داده‌های تولید شده توسط همان دو 
\lr{SCM}
بخش \eqref{toy} اجرا می‌کنیم. مجدداْ‌ الگوریتم هر دو اسکلت را به درستی تخمین می‌زند.

\subsection{اعمال الگوریتم پایدار بر داده‌های ضمیمه‌شده}
با الگوریتم 
\lr{pc-stable}
اسکلت گراف مولد داده‌ها را تخمین می‌زنیم. این الگوریتم زمان بیشتری نسبت به الگوریتم 
\lr{pc}
می‌گیرد و در نهایت در آخر سطح ۶ متوقف شده و ۳۱۳ یال باقی می‌ماند. 
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{stable.png}
	\caption{خروجی الگوریتم پایدار}
	\label{out2}
\end{figure}

\section{بخش چهارم - بررسی اثر درصد اطمینان}
در این بخش درصد اطمینان را در بازه‌ی 
$2^{-4}\%$
تا 
$2\%$
تغییر می‌دهیم و برای هر بازه‌ی اطمینان، ۲۰۰ بار الگوریتم مدنظر را اجرا می‌کنیم. در هر اجرا دو کمیت زیر را محاسبه می‌کنیم.
\begin{enumerate}
	\item \lr{Recall}
	نسبت تعداد‌ یال‌های گراف واقعی که درست تشخیص داده‌شده به  تعداد کل یال‌های گراف واقعی
	\item \lr{Missing}
	نسبت تعداد یال‌هایی که ما تشخیص دادیم درحالی که در گراف اصلی نبوده به  تعداد کل یال‌های گراف واقعی
\end{enumerate}

با افزایش ترشهولد، مشاهده می‌شود که 
\lr{Recall}
و
\lr{Missing}
افزایش می‌یابد. این موضوع بدیهی است زیرا با زیاد کردن ترشهولد، با سخت‌گیری کمتری یال‌ها را حذف می‌کنیم پس در‌کل تعداد یال‌های حذف شده زیاد‌ می‌شود و می‌توان به یال‌های باقی‌مانده اطمینان بیشتری داشت. دو نمودار زیر، نتایج این بخش هستند:

\begin{figure}[h]
	\begin{floatrow}
		\centering
		\includegraphics[scale=0.451]{plt1.png}
		\includegraphics[scale=0.451]{plt2.png}
	\end{floatrow}
	\caption{مقایسه‌ی دو الگوریتم}
	\label{wait}
\end{figure}
الگوریتم پایدار عملکرد بهتری داشته زیرا در الگوریتم 
\lr{pc}
معمولی چندین مشکل وجود دارد. اول اینکه با تغییر دادن ترتیب آزمون‌ها، نتایج متفاوت خواهد بود. دوم اینکه با در صورت یک خطا در آزمون، تعدادی آزمون دیگر انجام نخواهند شد. این دو مشکل تا حد زیادی در 
\lr{pc-stable} 
حل شده است. به همین دلیل تعداد یال‌های حذف‌شده در روش پایدار بیشتر از روش معمولی است.
مشاهده می‌شود در هر دو الگوریتم، با کاهش تعداد رئوس، نتایج پیشرفت قابل ملاحظه‌ای می‌کنند. دلیل این امر این است که در گراف‌های کوچک، خطا فرصت انتشار ندارد. این بهبود تا جایی است که برای گراف‌هایی با تعداد راس کمتر از ۱۵، 
\lr{Recall}
تقریباً برابر یک است و 
\lr{Missing}
مساوی صفر.

\end{document}